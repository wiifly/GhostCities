{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/CQ.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "print(df.head())\n",
        "\n",
        "# Split the 'Lines' column by semicolon and explode the DataFrame\n",
        "df['Lines'] = df['Lines'].str.split(';')\n",
        "print(df.head())\n",
        "df = df.explode('Lines')\n",
        "print(df.head(20))\n",
        "\n",
        "\n",
        "# Save the new DataFrame to a new CSV file\n",
        "new_file_path = '/content/cqStations.csv'\n",
        "df.to_csv(new_file_path, index=False)\n",
        "\n",
        "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Expanded Subway Stations DataFrame\", dataframe=df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiwUjlsbpKzv",
        "outputId": "cc48b53d-8efb-4c4b-c1a4-79324b29c131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Station     Lines        long        lat  Construction\n",
            "0     两路口   1号线;3号线  106.549342  29.552621             0\n",
            "1     石油路       1号线  106.508308  29.538110             0\n",
            "2      大坪   1号线;2号线  106.519184  29.541886             0\n",
            "3     富华路  18号线;9号线  106.503045  29.551102             1\n",
            "4      华村       9号线  106.520043  29.550587             1\n",
            "  Station        Lines        long        lat  Construction\n",
            "0     两路口   [1号线, 3号线]  106.549342  29.552621             0\n",
            "1     石油路        [1号线]  106.508308  29.538110             0\n",
            "2      大坪   [1号线, 2号线]  106.519184  29.541886             0\n",
            "3     富华路  [18号线, 9号线]  106.503045  29.551102             1\n",
            "4      华村        [9号线]  106.520043  29.550587             1\n",
            "   Station Lines        long        lat  Construction\n",
            "0      两路口   1号线  106.549342  29.552621             0\n",
            "0      两路口   3号线  106.549342  29.552621             0\n",
            "1      石油路   1号线  106.508308  29.538110             0\n",
            "2       大坪   1号线  106.519184  29.541886             0\n",
            "2       大坪   2号线  106.519184  29.541886             0\n",
            "3      富华路  18号线  106.503045  29.551102             1\n",
            "3      富华路   9号线  106.503045  29.551102             1\n",
            "4       华村   9号线  106.520043  29.550587             1\n",
            "5      佛图关   2号线  106.531503  29.549345             0\n",
            "6       鹅岭   1号线  106.533923  29.547986             0\n",
            "7      李子坝   2号线  106.537660  29.552892             0\n",
            "8      牛角沱   2号线  106.542605  29.560679             0\n",
            "8      牛角沱   3号线  106.542605  29.560679             0\n",
            "9      曾家岩  10号线  106.550111  29.566468             1\n",
            "9      曾家岩   2号线  106.550111  29.566468             1\n",
            "10     大礼堂  10号线  106.555377  29.561733             1\n",
            "11     七星岗  10号线  106.563142  29.554871             1\n",
            "11     七星岗   1号线  106.563142  29.554871             1\n",
            "12     大溪沟   2号线  106.558397  29.566175             0\n",
            "13     黄花园   2号线  106.564839  29.563396             0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    import requests\n",
        "    import json\n",
        "    from shapely import LineString\n",
        "    import geopandas as gpd\n",
        "    import transbigdata\n",
        "    import pandas as pd\n",
        "    url = 'https://map.amap.com/service/subway?_1707368894338&srhdata=3100_drw_shanghai.json'\n",
        "    headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 '\n",
        "    'Safari/537.36 Edg/121.0.0.0'\n",
        "    }\n",
        "    response = requests.get(url=url, headers=headers)\n",
        "    result = json.loads(response.text)\n",
        "    stop_data = []\n",
        "    line_data = gpd.GeoDataFrame()\n",
        "    names = []\n",
        "    lon_lat_s = []\n",
        "    for item in result['l']:\n",
        "        line_name = item['kn']\n",
        "        line_direction = '({start}-{end})'\n",
        "        start = item['st'][0]['n']\n",
        "        end = item['st'][-1]['n']\n",
        "\n",
        "        lon_lat = []\n",
        "        names.append(line_name)\n",
        "        x = item['x']\n",
        "        total = len(item['st'])\n",
        "        num = 1\n",
        "\n",
        "    for sub_item in item['st']:\n",
        "        n_data = sub_item['n']\n",
        "        sl_data = sub_item['sl']\n",
        "        sl_data_parts = sl_data.split(',')\n",
        "        sl_data_A = sl_data_parts[0]\n",
        "        sl_data_B = sl_data_parts[1]\n",
        "        lon_lat.append(sl_data_parts)\n",
        "        extracted_item_0 = {\n",
        "            'name': n_data,\n",
        "            'linename': line_name + line_direction.format(start=start, end=end),\n",
        "            'lon': sl_data_A,\n",
        "            'lat': sl_data_B,\n",
        "            'x': x,\n",
        "            'num': num,\n",
        "            'direction': 1\n",
        "        }\n",
        "        extracted_item_1 = {\n",
        "            'name': n_data,\n",
        "            'linename': line_name + line_direction.format(start=end, end=start),\n",
        "            'lon': sl_data_A,\n",
        "            'lat': sl_data_B,\n",
        "            'x': x,\n",
        "            'num': total-num+1,\n",
        "            'direction': 2\n",
        "        }\n",
        "        stop_data.append(extracted_item_0)\n",
        "        stop_data.append(extracted_item_1)\n",
        "        num += 1\n",
        "        lon_lat_s.append(LineString(lon_lat))\n",
        "    json_output = json.dumps(stop_data, indent=4)\n",
        "    with open('stop.json', 'w', encoding='utf-8') as f:\n",
        "        f.write(json_output)\n",
        "    stop = pd.read_json(r'stop.json', encoding='utf-8')\n",
        "    stop = stop.sort_values(by=['direction', 'x', 'num'])\n",
        "    stop.to_json(r'stop.json')\n",
        "    line_data['name'] = names\n",
        "    line_data.set_geometry(lon_lat_s, inplace=True)\n",
        "    line_data.to_file(\"line.shp\", encoding='utf-8')"
      ],
      "metadata": {
        "id": "vX4C5Uwkany3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "b4c92e01-8560-49e5-fce9-5a088b20ffa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 56)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m56\u001b[0m\n\u001b[0;31m    lon_lat_s.append(LineString(lon_lat))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lYynVF1-0Dn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwCnNz3LBXJX",
        "outputId": "efb2ea70-e5d1-444b-8fa1-f9150e451d24",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0     20190724\n",
            "1     20190725\n",
            "2     20190726\n",
            "3     20190727\n",
            "4     20190728\n",
            "5     20190729\n",
            "6     20190730\n",
            "7     20190805\n",
            "8     20190806\n",
            "9     20190807\n",
            "10    20190808\n",
            "11    20190809\n",
            "12    20190810\n",
            "13    20190811\n",
            "dtype: int64\n",
            "===Current City: shanghai ===\n",
            "  flag   date_dt  start_grid      end_grid  pop  odleng\n",
            "0   全部  20190724   310100166    3101007038  9.0     8.8\n",
            "1   全部  20190724   310100166   31010044923  4.0    30.0\n",
            "2   全部  20190724   310100166  310100110080  4.0    60.4\n",
            "3   全部  20190724  3101001083    3101009291  4.0     4.5\n",
            "4   全部  20190724  3101001083  310100133403  4.0    75.2\n",
            "Index(['flag', 'date_dt', 'start_grid', 'end_grid', 'pop', 'odleng'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import dask\n",
        "import dask.dataframe as dd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "folderPath = '/content/drive/MyDrive/Ghost_Cities_Research/'\n",
        "cities = ['shanghai', 'beijing', 'guangzhou', 'chongqing']\n",
        "groups = [u'全部', u'18岁以下', u'60岁以上', u'低收入人群']\n",
        "\"\"\"paths = ['/content/drive/MyDrive/Ghost_Cities_Research/odpf_shanghai.txt',\n",
        "         '/content/drive/MyDrive/Ghost_Cities_Research/odpf_beijing.txt',\n",
        "         '/content/drive/MyDrive/Ghost_Cities_Research/odpf_guangzhou.txt',\n",
        "         '/content/drive/MyDrive/Ghost_Cities_Research/odpf_chongqing.txt']\n",
        "\"\"\"\n",
        "heatwave_dates = pd.read_csv(folderPath + 'heatwave_dates.csv')\n",
        "#print(heatwave_dates.head)\n",
        "city = cities[0]\n",
        "group = groups[0]\n",
        "dates = heatwave_dates[heatwave_dates['city']==city]['date_dt']\n",
        "dates.name = None\n",
        "print(dates)\n",
        "\n",
        "#Convert large files to parquet (comment after the 1st run)\n",
        "#odFilepath = folderPath + 'odpf_' + city +'.txt'\n",
        "#df = dd.read_csv(odFilepath, delimiter='\\t')\n",
        "#df.to_parquet(folderPath + 'odpf_' + city + '(parquet)')\n",
        "\n",
        "\n",
        "\n",
        "print('===Current City:', city, '===')\n",
        "df = dd.read_parquet(folderPath + 'odpf_' + city + '(parquet)')\n",
        "print(df.head())\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# outflows, inflows without odlength\n",
        "outflows = df[df['flag']==group][['start_grid','date_dt','pop','odleng']]\\\n",
        "    .groupby(['start_grid', 'date_dt'])\\\n",
        "    .agg({'pop': 'sum'})\\\n",
        "    .compute().reset_index()\n",
        "inflows = df[df['flag']==group][['end_grid','date_dt','pop','odleng']]\\\n",
        "    .groupby(['end_grid', 'date_dt'])\\\n",
        "    .agg({'pop': 'sum'})\\\n",
        "    .compute().reset_index()\n",
        "print(outflows.head())\n",
        "print(inflows.head())\n",
        "\n",
        "outIDs = outflows['start_grid' ].unique()\n",
        "outdf = pd.DataFrame(columns = dates, index = outIDs)\n",
        "inIDs = inflows['end_grid'].unique()\n",
        "indf = pd.DataFrame(columns = dates, index = inIDs)\n",
        "\n",
        "outdf.index.name = 'start_grid'\n",
        "indf.index.name = 'end_grid'\n",
        "print(outdf.head())\n",
        "#print(indf.head())\n",
        "\n",
        "for index, row in outflows.iterrows():\n",
        "    date = row['date_dt']\n",
        "    pop = row['pop']\n",
        "    gridid = row['start_grid']\n",
        "    outdf.at[gridid, date] = pop\n",
        "    #print(date, pop, gridid)\n",
        "    #print(outdf.head())\n",
        "\n",
        "for index, row in inflows.iterrows():\n",
        "    date = row['date_dt']\n",
        "    pop = row['pop']\n",
        "    gridid = row['end_grid']\n",
        "    indf.at[gridid, date] = pop\n",
        "    #print(date, pop, gridid)\n",
        "    #print(indf.head())\n",
        "\n",
        "\n",
        "\n",
        "#newdf.reset_index(inplace=True)\n",
        "print(outdf.head(10))\n",
        "print(indf.head(10))\n",
        "#print(newdf.loc[3101002901])\n",
        "outdf['AvgPop'] = outdf.mean(axis=1)\n",
        "indf['AvgPop'] = indf.mean(axis=1)\n",
        "print('out:', outdf.head())\n",
        "print('in:' , indf.head())\n",
        "outdf.to_csv(folderPath + 'normalized_outflows_' + city + '.csv')\n",
        "indf.to_csv(folderPath + 'normalized_inflows_' + city + '.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Hqnb5vBgCa5f",
        "outputId": "356b4a4e-cea0-4ade-9dd7-34f25018ee62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   start_grid   date_dt     pop\n",
            "0   310100166  20190724  4268.0\n",
            "1  3101001083  20190724  1415.0\n",
            "2  3101001538  20190724   392.0\n",
            "3  3101001549  20190724  5735.0\n",
            "4  3101001989  20190724  7159.0\n",
            "       end_grid   date_dt      pop\n",
            "0    3101007038  20190724   1978.0\n",
            "1   31010044923  20190724   3644.0\n",
            "2  310100110080  20190724   5202.0\n",
            "3    3101009291  20190724   4727.0\n",
            "4  310100133403  20190724  33739.0\n",
            "           20190724 20190725 20190726 20190727 20190728 20190729 20190730  \\\n",
            "start_grid                                                                  \n",
            "310100166       NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
            "3101001083      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
            "3101001538      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
            "3101001549      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
            "3101001989      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
            "\n",
            "           20190805 20190806 20190807 20190808 20190809 20190810 20190811  \n",
            "start_grid                                                                 \n",
            "310100166       NaN      NaN      NaN      NaN      NaN      NaN      NaN  \n",
            "3101001083      NaN      NaN      NaN      NaN      NaN      NaN      NaN  \n",
            "3101001538      NaN      NaN      NaN      NaN      NaN      NaN      NaN  \n",
            "3101001549      NaN      NaN      NaN      NaN      NaN      NaN      NaN  \n",
            "3101001989      NaN      NaN      NaN      NaN      NaN      NaN      NaN  \n",
            "           20190724 20190725 20190726 20190727 20190728 20190729 20190730  \\\n",
            "start_grid                                                                  \n",
            "310100166    4268.0   4991.0   5169.0   4117.0   4672.0   4434.0   4715.0   \n",
            "3101001083   1415.0   1568.0   1504.0   1633.0   1529.0   1388.0   1601.0   \n",
            "3101001538    392.0    406.0      NaN      NaN      NaN      NaN      NaN   \n",
            "3101001549   5735.0   7035.0   7399.0   7448.0   6755.0   7009.0   6566.0   \n",
            "3101001989   7159.0   7893.0   7767.0   5810.0   5555.0   6372.0   6384.0   \n",
            "3101001995   3231.0   3631.0   3671.0   2712.0   2488.0   3554.0   3461.0   \n",
            "3101002003   4966.0   6661.0   6824.0   6498.0   5724.0   6489.0   6233.0   \n",
            "3101002446    745.0    676.0    645.0    420.0    433.0    396.0    494.0   \n",
            "3101002905    222.0    194.0    248.0    135.0    154.0    217.0    264.0   \n",
            "3101003366    107.0    127.0     88.0     94.0     65.0    114.0    136.0   \n",
            "\n",
            "           20190805 20190806 20190807 20190808 20190809 20190810 20190811  \n",
            "start_grid                                                                 \n",
            "310100166    2930.0   2699.0   3125.0   3029.0   2448.0   1317.0   2590.0  \n",
            "3101001083    209.0    215.0    260.0    346.0    226.0     87.0     83.0  \n",
            "3101001538      NaN      NaN      NaN      NaN      NaN      NaN      NaN  \n",
            "3101001549      NaN      NaN      NaN      NaN      NaN      NaN      NaN  \n",
            "3101001989   1039.0   1001.0   1179.0    956.0    890.0    499.0   1016.0  \n",
            "3101001995    623.0    629.0    593.0    597.0    550.0    238.0    323.0  \n",
            "3101002003    441.0    411.0    217.0    257.0    227.0     77.0    147.0  \n",
            "3101002446    408.0    463.0    604.0    665.0    581.0    206.0    352.0  \n",
            "3101002905    205.0    191.0    174.0    199.0    178.0    110.0    237.0  \n",
            "3101003366    135.0      NaN      NaN      NaN      NaN      NaN      NaN  \n",
            "             20190724 20190725 20190726 20190727 20190728 20190729 20190730  \\\n",
            "end_grid                                                                      \n",
            "3101007038     1978.0   2580.0   2599.0   2909.0   2873.0   2646.0   2550.0   \n",
            "31010044923    3644.0   4164.0   4173.0   4318.0   4364.0   4027.0   4142.0   \n",
            "310100110080   5202.0   4650.0   4663.0   1336.0    915.0   4364.0   4222.0   \n",
            "3101009291     4727.0   6931.0   6956.0   6718.0   6254.0   6355.0   6339.0   \n",
            "310100133403  33739.0  36381.0  35774.0  29820.0  27582.0  33093.0  33113.0   \n",
            "3101002446      597.0    539.0    518.0    411.0    366.0    350.0    476.0   \n",
            "31010039746    1242.0   1695.0   2088.0   1862.0      NaN      NaN      NaN   \n",
            "31010046284    2736.0   2677.0   2682.0   2662.0   2588.0   2484.0   2464.0   \n",
            "31010077260    3770.0   3944.0   3947.0   2337.0   2151.0   3838.0   3783.0   \n",
            "310100104307   2933.0   2679.0   2683.0   2490.0   2684.0   2925.0   2671.0   \n",
            "\n",
            "             20190805 20190806 20190807 20190808 20190809 20190810 20190811  \n",
            "end_grid                                                                     \n",
            "3101007038     2752.0   2773.0   2889.0   2690.0   2328.0   1454.0   2417.0  \n",
            "31010044923    4226.0   4340.0   4273.0   4282.0   3512.0   2686.0   3983.0  \n",
            "310100110080   4392.0   4579.0   4378.0   4554.0   3972.0    382.0    459.0  \n",
            "3101009291     4191.0   4632.0   4981.0   3271.0   2514.0   2274.0   3165.0  \n",
            "310100133403   4823.0   4007.0   3838.0   3254.0   2887.0   2019.0   3130.0  \n",
            "3101002446      361.0    410.0    545.0    707.0    505.0    191.0    406.0  \n",
            "31010039746    1563.0   1610.0      NaN      NaN      NaN    701.0     70.0  \n",
            "31010046284    1176.0   1132.0   1121.0   1073.0   1086.0    732.0    828.0  \n",
            "31010077260    3894.0   3803.0   3891.0   3623.0   3364.0   1501.0   1912.0  \n",
            "310100104307   3772.0   3849.0   3905.0   3711.0   2236.0    832.0   1013.0  \n",
            "out:            20190724 20190725 20190726 20190727 20190728 20190729 20190730  \\\n",
            "start_grid                                                                  \n",
            "310100166    4268.0   4991.0   5169.0   4117.0   4672.0   4434.0   4715.0   \n",
            "3101001083   1415.0   1568.0   1504.0   1633.0   1529.0   1388.0   1601.0   \n",
            "3101001538    392.0    406.0      NaN      NaN      NaN      NaN      NaN   \n",
            "3101001549   5735.0   7035.0   7399.0   7448.0   6755.0   7009.0   6566.0   \n",
            "3101001989   7159.0   7893.0   7767.0   5810.0   5555.0   6372.0   6384.0   \n",
            "\n",
            "           20190805 20190806 20190807 20190808 20190809 20190810 20190811  \\\n",
            "start_grid                                                                  \n",
            "310100166    2930.0   2699.0   3125.0   3029.0   2448.0   1317.0   2590.0   \n",
            "3101001083    209.0    215.0    260.0    346.0    226.0     87.0     83.0   \n",
            "3101001538      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
            "3101001549      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
            "3101001989   1039.0   1001.0   1179.0    956.0    890.0    499.0   1016.0   \n",
            "\n",
            "                 AvgPop  \n",
            "start_grid               \n",
            "310100166   3607.428571  \n",
            "3101001083   861.714286  \n",
            "3101001538        399.0  \n",
            "3101001549  6849.571429  \n",
            "3101001989  3822.857143  \n",
            "in:              20190724 20190725 20190726 20190727 20190728 20190729 20190730  \\\n",
            "end_grid                                                                      \n",
            "3101007038     1978.0   2580.0   2599.0   2909.0   2873.0   2646.0   2550.0   \n",
            "31010044923    3644.0   4164.0   4173.0   4318.0   4364.0   4027.0   4142.0   \n",
            "310100110080   5202.0   4650.0   4663.0   1336.0    915.0   4364.0   4222.0   \n",
            "3101009291     4727.0   6931.0   6956.0   6718.0   6254.0   6355.0   6339.0   \n",
            "310100133403  33739.0  36381.0  35774.0  29820.0  27582.0  33093.0  33113.0   \n",
            "\n",
            "             20190805 20190806 20190807 20190808 20190809 20190810 20190811  \\\n",
            "end_grid                                                                      \n",
            "3101007038     2752.0   2773.0   2889.0   2690.0   2328.0   1454.0   2417.0   \n",
            "31010044923    4226.0   4340.0   4273.0   4282.0   3512.0   2686.0   3983.0   \n",
            "310100110080   4392.0   4579.0   4378.0   4554.0   3972.0    382.0    459.0   \n",
            "3101009291     4191.0   4632.0   4981.0   3271.0   2514.0   2274.0   3165.0   \n",
            "310100133403   4823.0   4007.0   3838.0   3254.0   2887.0   2019.0   3130.0   \n",
            "\n",
            "                    AvgPop  \n",
            "end_grid                    \n",
            "3101007038     2531.285714  \n",
            "31010044923    4009.571429  \n",
            "310100110080   3433.428571  \n",
            "3101009291     4950.571429  \n",
            "310100133403  18104.285714  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outdf.to_csv(folderPath + 'normalized_outflows_' + city + '.csv')\n",
        "indf.to_csv(folderPath + 'normalized_inflows_' + city + '.csv')"
      ],
      "metadata": {
        "id": "LHVc5165iqS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outflows = df[df['flag']==group][['start_grid','date_dt','pop','odleng']]\\\n",
        "    .groupby(['start_grid', 'date_dt'])\\\n",
        "    .agg({'pop': 'sum', 'odleng': 'mean'})\\\n",
        "    .compute().reset_index()\n",
        "inflows = df[df['flag']==group][['end_grid','date_dt','pop','odleng']]\\\n",
        "    .groupby(['end_grid', 'date_dt'])\\\n",
        "    .agg({'pop': 'sum', 'odleng': 'mean'})\\\n",
        "    .compute().reset_index()\n",
        "\n",
        "print(outflows.head())\n",
        "print(inflows.head())\n",
        "#outflows.to_csv(folderPath + 'outflows_' + group + '_' + city + '.csv')\n",
        "#inflows.to_csv(folderPath + 'inflows_' + group + '_' + city + '.csv')\n",
        "\"\"\"\n",
        "outflowAvg = outflows.groupby(['start_grid']).mean()\n",
        "inflowAvg = inflows.groupby(['end_grid', 'date_dt']).mean()\n",
        "\n",
        "print(outflowAvg.head())\n",
        "print(inflowAvg.head())\n",
        "#  get average values for each start grid in outflows\n",
        "\"\"\"\n",
        "\n",
        "#outflows_dt = pd.merge(outflows, heatwave_dates, on='date_dt')\n",
        "#print(outflows_dt.head())\n",
        "#inflows_dt = pd.merge(inflows, heatwave_dates, on='date_dt')\n",
        "#print(inflows_dt.head())"
      ],
      "metadata": {
        "id": "zfy1yp4KQXhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outflows2 = df[df['flag']==group][['start_grid','date_dt','pop']]\\\n",
        "  .groupby(['start_grid', 'date_dt'])\\\n",
        "  .agg({'pop': 'sum'}).head()\n"
      ],
      "metadata": {
        "id": "iIf6jdGJX6kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "citygrid = gpd.read_file(folderPath + 'Grid/' + city + '_grid.gpkg')\n",
        "citygrid = citygrid[['gridid', 'geometry']]\n",
        "print(citygrid.head())\n",
        "citygrid['gridid'] = citygrid['gridid'].astype(int)\n",
        "citygrid['num_out'] = 0\n",
        "citygrid['num_in'] = 0\n",
        "citygrid['len_out'] = np.ndarray(shape=(len(citygrid)))\n",
        "citygrid['len_in'] = np.ndarray(shape=(len(citygrid)))\n",
        "citygrid['pop'] = np.ndarray(shape=(len(citygrid)))\n",
        "\n",
        "print(citygrid.head())\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "outflows = df[df['flag']==group][['start_grid','date_dt','pop']]\\\n",
        "  .groupby(['start_grid', 'date_dt'])\\\n",
        "  .agg({'pop': 'sum'})\\\n",
        "  .compute().reset_index()\n",
        "\n",
        "inflows = df[df['flag']==group][['end_grid','date_dt','pop']]\\\n",
        "  .groupby(['end_grid', 'date_dt'])\\\n",
        "  .agg({'pop': 'sum'})\\\n",
        "  .compute().reset_index()\n",
        "\n",
        "print(outflows.head())\n",
        "print(inflows.head())\n",
        "\n",
        "# outflows.head()\n",
        "outflows_dt = pd.merge(outflows, heatwave_dates, on='date_dt')\n",
        "print(outflows_dt.head())\n",
        "\n",
        "inflows_dt = pd.merge(inflows, heatwave_dates, on='date_dt')\n",
        "print(inflows_dt.head())\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "i = 0\n",
        "\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    start = row['start_grid']\n",
        "    end = row['end_grid']\n",
        "    #print(start, end)\n",
        "    i += 1\n",
        "    #print(i)\n",
        "    citygrid.loc[citygrid['gridid'] == start, 'num_out'] += 1\n",
        "    citygrid.loc[citygrid['gridid'] == end, 'num_in'] += 1\n",
        "    citygrid.loc[citygrid['gridid'] == start, 'len_out'].append(row['len_out'])\n",
        "    citygrid.loc[citygrid['gridid'] == end, 'len_in'].append(row['len_in'])\n",
        "    citygrid.loc[citygrid['gridid'] == start, 'pop'].append(row['pop'])\n",
        "    citygrid.loc[citygrid['gridid'] == end, 'pop'].append(row['pop'])\n",
        "\n",
        "    if i > 1000000:\n",
        "        break\n",
        "\n",
        "citygrid.head(20)\n",
        "citygrid.to_csv(folderPath + 'grid_flows' + city)\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "for group in groups:\n",
        "    newDf = df[df['flag'] == group[0]]\n",
        "    print(newDf.head())\n",
        "    newDf = newDf.drop(['flag'], axis=1)\n",
        "\n",
        "    print(newDf.head())\n",
        "    newDf['pop'] = newDf['pop'].astype('int32')\n",
        "    #newDf.to_parquet(folderPath + 'odpf_' + group[1] + '_' + city)\n",
        "    #newDf.to_csv(folderPath + 'odpf_' + group[1] + '_' + city)\n",
        "    #newDf.to_csv(folderPath + 'odpf_' + group[1] + '_' + city + '(tsv)', sep='/t')\n",
        "    break\n",
        "\"\"\"\n",
        "#separate different groups?\n",
        "\"\"\"\n",
        "#perhaps not necessary\n",
        "grid = gpd.read_file(folderPath + 'Grid/' + city + '_grid.gpkg')\n",
        "print(grid.head())\n",
        "gridIDs = grid['gridid']\n",
        "print(type(gridIDs))\n",
        "print(gridIDs)\n",
        "\n",
        "currGroup = groups[0]\n",
        "\n",
        "cityDates = heatwave_dates[heatwave_dates['city'] == city]\n",
        "\n",
        "groupOD = pd.DataFrame(columns = , index = gridIDs)\n",
        "print(groupOD.head())\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fT-WN0FbEg5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# June 22 CSV Normalization\n"
      ],
      "metadata": {
        "id": "XmqVNLiPIG5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# John Nabors June 22, 2024\n",
        "\"\"\"\n",
        "Trying to normalize the dataset so that there is only one row containing each gridID\n",
        "Create a new datframe with each date as a column, like a timeseries\n",
        "\n",
        "separate data for those under 18, over 60, or in low income groups will be moved to separate files\n",
        "\"\"\"\n",
        "files = [('Shanghai'  , '/content/shnydx_frequency_sh_2019.csv'),\n",
        "         ('Beijing'   , '/content/shnydx_frequency_bj_2019.csv'),\n",
        "         ('Guangzhou' , '/content/shnydx_frequency_gz_2019.csv'),\n",
        "         ('Chongqing' , '/content/shnydx_frequency_cq_2019.csv')]\n",
        "\n",
        "\n",
        "for city_name, file_name in files:\n",
        "    print('===Current City:', city_name, '===')\n",
        "    tripFreq = pd.read_csv(file_name)\n",
        "\n",
        "    dates   = tripFreq[' date_dt'].unique()\n",
        "    gridIDs = tripFreq[' gridid' ].unique()\n",
        "    flags = ['18岁以下', '60岁以上', '低收入人群', '全部']\n",
        "    flagsEN = ['Under18', 'Over60', 'LowIncome', 'All']\n",
        "    #print(\"Dates: \", dates)\n",
        "\n",
        "    for currFlag in flags[::-1]:\n",
        "        print('Current Flag:', currFlag)\n",
        "\n",
        "        flag_filtered = tripFreq[tripFreq['flag'] == currFlag]\n",
        "        newdf = pd.DataFrame(columns=dates, index = gridIDs)\n",
        "        newdf.index.name = 'gridid'\n",
        "\n",
        "        for index, row in flag_filtered.iterrows():\n",
        "            date = row[' date_dt']\n",
        "            tripFrequency = row[' tripfrequency']\n",
        "            gridid = row[' gridid']\n",
        "            #print('date: ', date, 'freq: ', tripFrequency, 'ID: ', gridid)\n",
        "            newdf.at[gridid, date] = tripFrequency\n",
        "        #newdf.reset_index(inplace=True)\n",
        "        print(newdf.head(10))\n",
        "        #print(newdf.loc[3101002901])\n",
        "        newdf.to_csv('/content/' + city_name + '_Timeseries_' + flagsEN.pop() +'.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "8gzVLaUJIGPW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}